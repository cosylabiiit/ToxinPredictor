{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309bbee5-0e1d-49c2-b143-18566db404ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 15:38:52.422902: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-19 15:38:52.442030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-19 15:38:52.463058: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-19 15:38:52.469353: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-19 15:38:52.485656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 15:38:53.609885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score, precision_score, recall_score, roc_curve, precision_recall_curve, matthews_corrcoef\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756dbbc0-a830-4895-98b8-d3df63801f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and preprocess dataset\n",
    "df = pd.read_csv('merged_data_clean.csv')\n",
    "df = df[(df['Source'] == 'ToxiM') | (df['Source'] == 'MolToxPred')]\n",
    "categorical_columns = ['SMILES', 'Source']\n",
    "df = df.drop(columns=categorical_columns)\n",
    "Y = df['Toxicity']\n",
    "X_pca_df = pd.read_csv('X_pca_clean_with_boruta_ToxiM_and_MolToxPred.csv')\n",
    "# X_pca_df = pd.read_csv('X_pca_clean_with_boruta_ToxiM_and_MolToxPred.csv')\n",
    "# Handle class imbalance using SMOTE\n",
    "# Split the original data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca_df, Y, test_size=0.2, random_state=42, shuffle=True, stratify=Y)\n",
    "\n",
    "# Apply SMOTE only on the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbbfb7e-8c4f-4257-a0b3-8d902ba24164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/hyperparameter_tuning_bayes/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39665/764455109.py:2: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import BayesianOptimization\n",
      "2024-09-19 15:40:34.079720: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/angadjeet22071/miniconda3/lib/python3.11/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6495 - loss: 5.6477 - val_accuracy: 0.7845 - val_loss: 4.4537 - learning_rate: 1.6803e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7750 - loss: 4.2110 - val_accuracy: 0.8072 - val_loss: 3.4740 - learning_rate: 1.6803e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8028 - loss: 3.2871 - val_accuracy: 0.8196 - val_loss: 2.7730 - learning_rate: 1.6803e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8306 - loss: 2.5906 - val_accuracy: 0.8183 - val_loss: 2.2739 - learning_rate: 1.6803e-04\n",
      "Epoch 5/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8379 - loss: 2.1325 - val_accuracy: 0.8223 - val_loss: 1.9347 - learning_rate: 1.6803e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8469 - loss: 1.7876 - val_accuracy: 0.8254 - val_loss: 1.6566 - learning_rate: 1.6803e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8570 - loss: 1.5243 - val_accuracy: 0.8307 - val_loss: 1.4483 - learning_rate: 1.6803e-04\n",
      "Epoch 8/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8627 - loss: 1.3180 - val_accuracy: 0.8285 - val_loss: 1.3075 - learning_rate: 1.6803e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8756 - loss: 1.1394 - val_accuracy: 0.8259 - val_loss: 1.2003 - learning_rate: 1.6803e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8794 - loss: 1.0211 - val_accuracy: 0.8272 - val_loss: 1.0957 - learning_rate: 1.6803e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8802 - loss: 0.9132 - val_accuracy: 0.8276 - val_loss: 1.0284 - learning_rate: 1.6803e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8843 - loss: 0.8296 - val_accuracy: 0.8241 - val_loss: 0.9716 - learning_rate: 1.6803e-04\n",
      "Epoch 13/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8895 - loss: 0.7648 - val_accuracy: 0.8259 - val_loss: 0.9228 - learning_rate: 1.6803e-04\n",
      "Epoch 14/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8998 - loss: 0.6840 - val_accuracy: 0.8267 - val_loss: 0.8727 - learning_rate: 1.6803e-04\n",
      "Epoch 15/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9013 - loss: 0.6432 - val_accuracy: 0.8276 - val_loss: 0.8411 - learning_rate: 1.6803e-04\n",
      "Epoch 16/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9048 - loss: 0.6006 - val_accuracy: 0.8267 - val_loss: 0.8140 - learning_rate: 1.6803e-04\n",
      "Epoch 17/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9070 - loss: 0.5569 - val_accuracy: 0.8316 - val_loss: 0.7847 - learning_rate: 1.6803e-04\n",
      "Epoch 18/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9117 - loss: 0.5294 - val_accuracy: 0.8245 - val_loss: 0.7636 - learning_rate: 1.6803e-04\n",
      "Epoch 19/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9080 - loss: 0.5128 - val_accuracy: 0.8223 - val_loss: 0.7485 - learning_rate: 1.6803e-04\n",
      "Epoch 20/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9162 - loss: 0.4778 - val_accuracy: 0.8290 - val_loss: 0.7446 - learning_rate: 1.6803e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9200 - loss: 0.4560 - val_accuracy: 0.8170 - val_loss: 0.7400 - learning_rate: 1.6803e-04\n",
      "Epoch 22/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9198 - loss: 0.4360 - val_accuracy: 0.8267 - val_loss: 0.7111 - learning_rate: 1.6803e-04\n",
      "Epoch 23/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9183 - loss: 0.4205 - val_accuracy: 0.8241 - val_loss: 0.7161 - learning_rate: 1.6803e-04\n",
      "Epoch 24/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9191 - loss: 0.4061 - val_accuracy: 0.8205 - val_loss: 0.7230 - learning_rate: 1.6803e-04\n",
      "Epoch 25/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9204 - loss: 0.4002 - val_accuracy: 0.8223 - val_loss: 0.7182 - learning_rate: 1.6803e-04\n",
      "Epoch 26/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9210 - loss: 0.3863 - val_accuracy: 0.8245 - val_loss: 0.6979 - learning_rate: 1.6803e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9263 - loss: 0.3701 - val_accuracy: 0.8263 - val_loss: 0.6873 - learning_rate: 1.6803e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9320 - loss: 0.3531 - val_accuracy: 0.8196 - val_loss: 0.7031 - learning_rate: 1.6803e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9370 - loss: 0.3378 - val_accuracy: 0.8227 - val_loss: 0.7124 - learning_rate: 1.6803e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9328 - loss: 0.3461 - val_accuracy: 0.8259 - val_loss: 0.7124 - learning_rate: 1.6803e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9303 - loss: 0.3333 - val_accuracy: 0.8165 - val_loss: 0.7032 - learning_rate: 1.6803e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9358 - loss: 0.3213 - val_accuracy: 0.8174 - val_loss: 0.6821 - learning_rate: 1.6803e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9277 - loss: 0.3339 - val_accuracy: 0.8227 - val_loss: 0.7033 - learning_rate: 1.6803e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9347 - loss: 0.3140 - val_accuracy: 0.8276 - val_loss: 0.7131 - learning_rate: 1.6803e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9378 - loss: 0.3028 - val_accuracy: 0.8267 - val_loss: 0.6911 - learning_rate: 1.6803e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9395 - loss: 0.2981 - val_accuracy: 0.8227 - val_loss: 0.7015 - learning_rate: 1.6803e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9378 - loss: 0.2981 - val_accuracy: 0.8219 - val_loss: 0.7330 - learning_rate: 1.6803e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9371 - loss: 0.3014 - val_accuracy: 0.8259 - val_loss: 0.6962 - learning_rate: 1.6803e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9411 - loss: 0.2865 - val_accuracy: 0.8267 - val_loss: 0.6863 - learning_rate: 1.6803e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9433 - loss: 0.2809 - val_accuracy: 0.8205 - val_loss: 0.6960 - learning_rate: 1.6803e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9410 - loss: 0.2869 - val_accuracy: 0.8196 - val_loss: 0.6884 - learning_rate: 1.6803e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9352 - loss: 0.2815 - val_accuracy: 0.8227 - val_loss: 0.6865 - learning_rate: 1.6803e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9450 - loss: 0.2684 - val_accuracy: 0.8267 - val_loss: 0.6903 - learning_rate: 1.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9527 - loss: 0.2498 - val_accuracy: 0.8276 - val_loss: 0.7374 - learning_rate: 1.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9521 - loss: 0.2445 - val_accuracy: 0.8227 - val_loss: 0.7337 - learning_rate: 1.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9503 - loss: 0.2361 - val_accuracy: 0.8241 - val_loss: 0.7297 - learning_rate: 1.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9522 - loss: 0.2366 - val_accuracy: 0.8303 - val_loss: 0.7427 - learning_rate: 1.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9548 - loss: 0.2271 - val_accuracy: 0.8250 - val_loss: 0.7305 - learning_rate: 1.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9551 - loss: 0.2261 - val_accuracy: 0.8254 - val_loss: 0.7484 - learning_rate: 1.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9499 - loss: 0.2260 - val_accuracy: 0.8272 - val_loss: 0.7602 - learning_rate: 1.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9588 - loss: 0.2149 - val_accuracy: 0.8254 - val_loss: 0.7395 - learning_rate: 1.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9572 - loss: 0.2087 - val_accuracy: 0.8245 - val_loss: 0.7362 - learning_rate: 1.0000e-04\n",
      "Deep Neural Network trained.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define and train Deep Neural Network with Keras Tuner\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 6)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), \n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', min_value=0.3, max_value=0.5, step=0.05)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = BayesianOptimization(build_model, \n",
    "                             objective='val_accuracy', \n",
    "                             max_trials=20, \n",
    "                             executions_per_trial=2, \n",
    "                             directory='tuner_dir', \n",
    "                             project_name='hyperparameter_tuning_bayes')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=50, validation_split=0.2, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "dnn_model = tuner.hypermodel.build(best_hps)\n",
    "history = dnn_model.fit(X_train, Y_train, epochs=300, batch_size=32, validation_split=0.2, verbose=1, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "print(\"Deep Neural Network trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255d1545-9dfd-4355-8df7-f186a75fe1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is running LR\n",
      "This model is running DT\n",
      "This model is running RF\n",
      "This model is running GB\n",
      "This model is running XGB\n",
      "This model is running SVM\n",
      "This model is running KNN\n",
      "This model is running NB\n",
      "Traditional machine learning models trained.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'LR': LogisticRegression(random_state=42,max_iter=200, C =0.01),\n",
    "    'DT': DecisionTreeClassifier(random_state = 42,max_depth=10),\n",
    "    'RF': RandomForestClassifier(random_state = 42, n_estimators=100),\n",
    "    'GB': GradientBoostingClassifier(random_state=42,learning_rate=0.2),\n",
    "    'XGB': XGBClassifier(random_state = 42,use_label_encoder=False, eval_metric='logloss', learning_rate=0.1),\n",
    "    'SVM': SVC(probability=True, C=1),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7),\n",
    "    'NB': GaussianNB()\n",
    "}\n",
    "\n",
    "fitted_models = {}\n",
    "for model_name, model in models.items():\n",
    "    # if model_name not in [\"SVM\"]:\n",
    "    #         continue\n",
    "    print(\"This model is running\", model_name)\n",
    "    model.fit(X_train, Y_train)  # Fit the model to the training data\n",
    "    fitted_models[model_name] = model  # Store the trained model\n",
    "fitted_models['DNN'] = dnn_model;\n",
    "print(\"Traditional machine learning models trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c446d90c-a590-4e9d-b944-ebd3cbfadbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is running: LR\n",
      "This model is running: DT\n",
      "This model is running: RF\n",
      "This model is running: GB\n",
      "This model is running: XGB\n",
      "This model is running: SVM\n",
      "This model is running: KNN\n",
      "This model is running: NB\n",
      "Traditional machine learning models trained.\n",
      "Fitted models saved to 'fitted_models.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "fitted_models = {}\n",
    "\n",
    "# Iterate over models, only fitting the \"SVM\" model\n",
    "for model_name, model in models.items():\n",
    "    # if model_name not in [\"SVM\"]:\n",
    "    #     continue\n",
    "    print(\"This model is running:\", model_name)\n",
    "    model.fit(X_train, Y_train)  # Fit the model to the training data\n",
    "    fitted_models[model_name] = model  # Store the trained model\n",
    "\n",
    "# Add the DNN model separately\n",
    "fitted_models['DNN'] = dnn_model\n",
    "\n",
    "print(\"Traditional machine learning models trained.\")\n",
    "\n",
    "# Save the fitted models to a file\n",
    "with open('fitted_models.pkl', 'wb') as file:\n",
    "    pickle.dump(fitted_models, file)\n",
    "\n",
    "print(\"Fitted models saved to 'fitted_models.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c70f2-6fb4-4561-ad0c-a91d48113ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f177344d-a07a-4a37-a8bf-3c2487996241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown model type.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# Assuming model is an instance of SVM\n",
    "if isinstance(model, SVC):\n",
    "    print(f\"Kernel used: {model.kernel}\")\n",
    "    if model.kernel == 'linear':\n",
    "        print(\"The SVM model is linear.\")\n",
    "    else:\n",
    "        print(\"The SVM model is non-linear.\")\n",
    "elif isinstance(model, LinearSVC):\n",
    "    print(\"The SVM model is linear.\")\n",
    "else:\n",
    "    print(\"Unknown model type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c597eb8-8226-4dce-84df-e142ab4c81d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DNN...\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Evaluating LR...\n",
      "Evaluating DT...\n",
      "Evaluating RF...\n",
      "Evaluating GB...\n",
      "Evaluating XGB...\n",
      "Evaluating SVM...\n",
      "Evaluating KNN...\n",
      "Evaluating NB...\n",
      "Model metrics saved to 'model_metrics.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Evaluate all models and store metrics\n",
    "def evaluate_models(models, X_train, X_test, Y_train, Y_test):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        if model_name == 'DNN':\n",
    "            train_pred = (model.predict(X_train) > 0.5).astype(int)\n",
    "            test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "            train_pred_prob = model.predict(X_train).ravel()\n",
    "            test_pred_prob = model.predict(X_test).ravel()\n",
    "        else:\n",
    "            train_pred = model.predict(X_train)\n",
    "            test_pred = model.predict(X_test)\n",
    "            train_pred_prob = model.predict_proba(X_train)[:, 1]\n",
    "            test_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Train Accuracy': accuracy_score(Y_train, train_pred),\n",
    "            'Test Accuracy': accuracy_score(Y_test, test_pred),\n",
    "            'Train F1': f1_score(Y_train, train_pred),\n",
    "            'Test F1': f1_score(Y_test, test_pred),\n",
    "            'Train MCC': matthews_corrcoef(Y_train, train_pred),\n",
    "            'Test MCC': matthews_corrcoef(Y_test, test_pred),\n",
    "            'Train ROC AUC': roc_auc_score(Y_train, train_pred_prob),\n",
    "            'Test ROC AUC': roc_auc_score(Y_test, test_pred_prob),\n",
    "            'Train Precision': precision_score(Y_train, train_pred),\n",
    "            'Test Precision': precision_score(Y_test, test_pred),\n",
    "            'Train Recall': recall_score(Y_train, train_pred),\n",
    "            'Test Recall': recall_score(Y_test, test_pred)\n",
    "        }\n",
    "        results.append(metrics)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"model_metrics_latest_good_smote.csv\")\n",
    "    print(\"Model metrics saved to 'model_metrics.csv'.\")\n",
    "\n",
    "evaluate_models({'DNN': dnn_model, **fitted_models}, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5820a0e7-c4e3-40b5-b76a-f772ba11ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import lime\n",
    "# from lime.lime_tabular import LimeTabularExplainer\n",
    "# import pandas as pd\n",
    "\n",
    "# print(\"Applying LIME analysis...\")\n",
    "\n",
    "# # Create a LIME explainer\n",
    "# explainer = LimeTabularExplainer(\n",
    "#     training_data=np.array(X_train),\n",
    "#     feature_names=X_pca_df.columns,\n",
    "#     class_names=['Non-Toxic', 'Toxic'],\n",
    "#     mode='classification'\n",
    "# )\n",
    "\n",
    "# # Select a test instance to explain\n",
    "# idx = 0  # Index of the instance you want to explain\n",
    "# instance = X_test.iloc[idx]  # Use .iloc for integer-location based indexing\n",
    "\n",
    "# # Define the predict function for each model to return probabilities\n",
    "# def predict_fn(x, model):\n",
    "#     preds = model.predict(x)\n",
    "#     # Normalize the output if necessary\n",
    "#     if len(preds.shape) == 1:  # Binary classification case for DNN\n",
    "#         preds = np.hstack((1 - preds.reshape(-1, 1), preds.reshape(-1, 1)))\n",
    "#     elif preds.shape[1] == 1:  # Binary classification case for other models\n",
    "#         preds = np.hstack((1 - preds, preds))\n",
    "#     return preds\n",
    "\n",
    "# # Function to apply LIME and generate explanations\n",
    "# def apply_lime_and_save(models, instance, explainer):\n",
    "#     for model_name, model in models.items():\n",
    "#         print(f\"Applying LIME for {model_name}...\")\n",
    "#         exp = explainer.explain_instance(\n",
    "#             data_row=instance,\n",
    "#             predict_fn=lambda x: predict_fn(x, model),\n",
    "#             num_features=10\n",
    "#         )\n",
    "#         exp.save_to_file(f'lime_explanation_{model_name}.html')\n",
    "#         print(f\"LIME explanation for {model_name} saved to 'lime_explanation_{model_name}.html'.\")\n",
    "\n",
    "# apply_lime_and_save({'DNN': dnn_model, **fitted_models}, instance, explainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05334bca-8ed8-44fb-be89-615056958e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import shap\n",
    "# import json\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Iterate through each fitted model to perform SHAP analysis\n",
    "# shap_values_dict = {}\n",
    "# processed_models = []\n",
    "\n",
    "# # Summarize the background data using k-means with fewer clusters\n",
    "# background = shap.kmeans(X_test, 5)  # Reducing to 5 clusters for faster computation\n",
    "\n",
    "# # Use a subset of X_test for SHAP analysis\n",
    "# X_test_subset = X_test.sample(n=1, random_state=42)  \n",
    "\n",
    "# for model_name, model in fitted_models.items():\n",
    "#     try:\n",
    "#         if model_name not in [ \"SVM\"]:\n",
    "#             continue\n",
    "#         print(f\"Performing SHAP analysis for {model_name}...\")\n",
    "\n",
    "#         # Use KernelExplainer for SVM\n",
    "#         if model_name == \"SVM\":\n",
    "#             explainer = shap.KernelExplainer(model.predict, background)\n",
    "#         else:\n",
    "#             explainer = shap.Explainer(model.predict, X_test)\n",
    "\n",
    "#         # Calculate SHAP values using the subset of X_test\n",
    "#         shap_values = explainer.shap_values(X_test_subset)\n",
    "\n",
    "#         # Store the SHAP values in a dictionary\n",
    "#         shap_values_dict[model_name] = shap_values\n",
    "\n",
    "#         # Save SHAP values to a file\n",
    "#         shap_file = f\"{model_name}_shap_values_pca.json\"\n",
    "#         # with open(shap_file, \"w\") as f:\n",
    "#         #     json.dump(shap_values.tolist(), f)  # Convert numpy arrays to lists for JSON serialization\n",
    "\n",
    "#         # Plot and save SHAP summary plot for each model\n",
    "#         plt.figure()  # Create a new figure\n",
    "#         shap.summary_plot(shap_values, X_test_subset, feature_names=X_test.columns)  # Prevent immediate display\n",
    "    \n",
    "#         plt.tight_layout()  # Adjust layout to fit everything\n",
    "#         # plt.savefig(f\"{model_name}_shap_summary_plot_from_json.png\", format='png', dpi=300, bbox_inches='tight')  # Save the plot as a PNG file\n",
    "#         plt.close()  # Close the plot to free up memory and avoid overlap\n",
    "#             # Add the model to the processed list\n",
    "#         processed_models.append(model_name)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while processing {model_name}: {e}\")\n",
    "\n",
    "# print(\"SHAP analysis completed for all models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf21c66-3189-43e3-bf43-c5f7195cc4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing SHAP analysis for SVM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852ba26f91df4c9eb0e7bf40538d58ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Iterate through each fitted model to perform SHAP analysis\n",
    "processed_models = []\n",
    "\n",
    "# Summarize the background data using k-means with fewer clusters\n",
    "background = shap.kmeans(X_test, 5)  # Reducing to 5 clusters for faster computation\n",
    "\n",
    "# Use a subset of X_test for SHAP analysis\n",
    "X_test_subset = X_test.sample(n=3, random_state=42)  \n",
    "\n",
    "for model_name, model in fitted_models.items():\n",
    "    try:\n",
    "        if model_name != \"SVM\":  # Only process the SVM model\n",
    "            continue\n",
    "        print(f\"Performing SHAP analysis for {model_name}...\")\n",
    "\n",
    "        # Use KernelExplainer for SVM\n",
    "        explainer = shap.KernelExplainer(model.predict, background)\n",
    "        \n",
    "        # Calculate SHAP values using the subset of X_test\n",
    "        shap_values = explainer.shap_values(X_test_subset)\n",
    "\n",
    "        # Generate and save SHAP summary plot\n",
    "        shap.summary_plot(shap_values, X_test_subset, feature_names=X_test.columns)\n",
    "        plt.savefig(f\"{model_name}_shap_summary_plot.png\")  # Save the plot as a PNG file\n",
    "        plt.close()  # Close the plot to avoid overlap in subsequent plots\n",
    "\n",
    "        # Add the model to the processed list\n",
    "        processed_models.append(model_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {model_name}: {e}\")\n",
    "\n",
    "print(\"SHAP analysis completed for all models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366b869-208d-4b0c-9932-c85fe56293d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load SHAP values from a JSON file\n",
    "def load_shap_values(json_file):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        shap_values_list = json.load(f)\n",
    "    return shap.Explanation(values=np.array(shap_values_list))\n",
    "\n",
    "# Example for a specific model\n",
    "model_name = \"SVM\"  # Replace with the actual model name\n",
    "shap_file = f\"{model_name}_shap_values_boruta.json\"\n",
    "\n",
    "# Sample the test data\n",
    "X_test_subset = X_test.sample(n=100, random_state=42)\n",
    "\n",
    "try:\n",
    "    shap_values = load_shap_values(shap_file)\n",
    "    \n",
    "    # Validate SHAP values\n",
    "    if shap_values is None or shap_values.values.size == 0:\n",
    "        raise ValueError(\"Loaded SHAP values are empty or not valid.\")\n",
    "    \n",
    "    # Ensure SHAP values and feature data compatibility\n",
    "    if X_test_subset.shape[1] != shap_values.values.shape[1]:\n",
    "        raise ValueError(\"Mismatch between SHAP values and feature data.\")\n",
    "    \n",
    "    # Generate and save SHAP summary plot\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values.values, X_test_subset, feature_names=X_test_subset.columns)\n",
    "    \n",
    "    # Save the plot as a PNG file\n",
    "    plot_file = f\"{model_name}_shap_summary_plot_from_json.png\"\n",
    "    plt.savefig(plot_file, format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"SHAP summary plot generated and saved as {plot_file}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a657e-dee4-41ce-ac52-a39a848e4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def load_shap_values_and_generate_plot(model_name, X_test):\n",
    "#     \"\"\"\n",
    "#     Load SHAP values from a JSON file and generate a SHAP summary plot.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_name: str, name of the model whose SHAP values are to be loaded\n",
    "#     - X_test: DataFrame, the feature matrix used for the SHAP analysis\n",
    "#     \"\"\"\n",
    "#     # Load SHAP values from the JSON file\n",
    "#     shap_file = f\"{model_name}_shap_values.json\"\n",
    "#     try:\n",
    "#         with open(shap_file, \"r\") as f:\n",
    "#             shap_values_list = json.load(f)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File {shap_file} not found for model {model_name}.\")\n",
    "#         return\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(f\"Error decoding JSON from {shap_file} for model {model_name}.\")\n",
    "#         return\n",
    "\n",
    "#     # Convert the loaded SHAP values list back to a NumPy array\n",
    "#     shap_values_array = np.array(shap_values_list)\n",
    " \n",
    "#     # Create SHAP values object (needed for plotting)\n",
    "#     shap_values = shap._explanation.Explanation(shap_values_array, base_values=None, data=X_test.values)\n",
    "\n",
    "#     # Generate and save the SHAP summary plot\n",
    "#     plt.figure()  # Create a new figure\n",
    "#     shap.summary_plot(shap_values, X_test, feature_names=X_test.columns, show=False)  # Prevent immediate display\n",
    "\n",
    "#     plt.tight_layout()  # Adjust layout to fit everything\n",
    "#     plt.savefig(f\"{model_name}_shap_summary_plot_from_json.png\", format='png', dpi=300, bbox_inches='tight')  # Save the plot as a PNG file\n",
    "#     plt.close()  # Close the plot to free up memory and avoid overlap\n",
    "\n",
    "#     print(f\"SHAP summary plot generated and saved for {model_name}.\")\n",
    "\n",
    "# # Example usage\n",
    "# # Iterate through the processed models and generate plots from the saved JSON files\n",
    "# for model_name in processed_models:\n",
    "#     load_shap_values_and_generate_plot(model_name, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0499e5-ebf3-45e7-8ff8-7fbe9b8f1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "# def plot_curves(models, X_train, X_test, Y_train, Y_test):\n",
    "#     plt.rcParams.update({'font.size': 12})\n",
    "    \n",
    "#     # ROC Curve for Training Data\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     for model_name, model in models.items():\n",
    "#         if model_name == 'DNN':\n",
    "#             train_pred_prob = model.predict(X_train).ravel()\n",
    "#         else:\n",
    "#             train_pred_prob = model.predict_proba(X_train)[:, 1]\n",
    "        \n",
    "#         fpr_train, tpr_train, _ = roc_curve(Y_train, train_pred_prob)\n",
    "#         plt.plot(fpr_train, tpr_train, label=f'{model_name}')\n",
    "    \n",
    "#     plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Diagonal 45-degree line\n",
    "#     plt.title('ROC Curve - Training Data')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.legend()\n",
    "#     plt.savefig('roc_curve_train.png', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "#     # ROC Curve for Testing Data\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     for model_name, model in models.items():\n",
    "#         if model_name == 'DNN':\n",
    "#             test_pred_prob = model.predict(X_test).ravel()\n",
    "#         else:\n",
    "#             test_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "#         fpr_test, tpr_test, _ = roc_curve(Y_test, test_pred_prob)\n",
    "#         plt.plot(fpr_test, tpr_test, label=f'{model_name}')\n",
    "    \n",
    "#     plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Diagonal 45-degree line\n",
    "#     plt.title('ROC Curve - Testing Data')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.legend()\n",
    "#     plt.savefig('roc_curve_test.png', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Precision-Recall Curve for Training Data\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     for model_name, model in models.items():\n",
    "#         if model_name == 'DNN':\n",
    "#             train_pred_prob = model.predict(X_train).ravel()\n",
    "#         else:\n",
    "#             train_pred_prob = model.predict_proba(X_train)[:, 1]\n",
    "        \n",
    "#         precision_train, recall_train, _ = precision_recall_curve(Y_train, train_pred_prob)\n",
    "#         plt.plot(recall_train, precision_train, label=f'{model_name}')\n",
    "#     # plt.plot([1, 0], [0, 1], linestyle='--', color='black')\n",
    "#     plt.title('Precision-Recall Curve - Training Data')\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.legend()\n",
    "#     plt.savefig('precision_recall_curve_train.png', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Precision-Recall Curve for Testing Data\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     for model_name, model in models.items():\n",
    "#         if model_name == 'DNN':\n",
    "#             test_pred_prob = model.predict(X_test).ravel()\n",
    "#         else:\n",
    "#             test_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "#         precision_test, recall_test, _ = precision_recall_curve(Y_test, test_pred_prob)\n",
    "#         plt.plot(recall_test, precision_test, label=f'{model_name}')\n",
    "#     # plt.plot([1, 0], [0, 1], linestyle='--', color='black')\n",
    "#     plt.title('Precision-Recall Curve - Testing Data')\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.legend()\n",
    "#     plt.savefig('precision_recall_curve_test.png', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "#     print(\"All plots have been saved as separate PNG files with 300 dpi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3cc86-5e1d-45d4-b2aa-929772491232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Plotting ROC and Precision-Recall curves...\")\n",
    "# plot_curves(fitted_models, X_train, X_test, Y_train, Y_test)\n",
    "# print(\"Plots saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
